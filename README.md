# Multi-Node Training with KubeRay

This project scaffolds a distributed LLM training environment using:

- ğŸ§  **KubeRay** for orchestrating distributed training across multiple nodes
- ğŸ§Š **JuiceFS** backed by DigitalOcean Spaces for shared dataset storage
- ğŸ¤— **Hugging Face Datasets** for raw language modeling data
- âš™ï¸ **Kubernetes** with AMD MI300X GPU nodes (or similar)

## ğŸ”§ Prerequisites

- A Kubernetes cluster with GPU nodes
- `kubectl` configured
- `awscli` for uploading to DigitalOcean Spaces
- Your Spaces credentials exported:
  ```bash
  export AWS_ACCESS_KEY_ID=your_key
  export AWS_SECRET_ACCESS_KEY=your_secret
  ```

## ğŸ“¦ Dataset Preparation & Upload

### Dataset Loader Script

```python
# data/dataset-loader.py
from datasets import load_dataset
import os

# Output directory for saving the raw dataset
output_dir = "/tmp/hf-dataset/wikitext-103-raw-v1"
os.makedirs(output_dir, exist_ok=True)

# Load and save the raw dataset
print("Downloading wikitext-103-raw-v1 from Hugging Face...")
ds = load_dataset("wikitext", "wikitext-103-raw-v1")
ds.save_to_disk(output_dir)

print(f"Dataset saved to: {output_dir}")
```

### Upload Script

```bash
# scripts/upload-to-spaces.sh
#!/bin/bash
# Usage: ./upload-to-spaces.sh <local-folder> <bucket-name> <target-path>

LOCAL_DIR=$1
BUCKET_NAME=$2
TARGET_PATH=$3
REGION="nyc3"
ENDPOINT="https://${REGION}.digitaloceanspaces.com"

# Make sure AWS credentials are exported or set up via ~/.aws/credentials
aws --endpoint-url $ENDPOINT s3 cp --recursive "$LOCAL_DIR" "s3://$BUCKET_NAME/$TARGET_PATH"
```

## ğŸš€ Getting Started

1. Run dataset loader:
   ```bash
   python data/dataset-loader.py
   ```

2. Upload to DigitalOcean Spaces:
   ```bash
   ./scripts/upload-to-spaces.sh /tmp/hf-dataset/wikitext-103-raw-v1 your-bucket-name datasets/wikitext
   ```

## ğŸ“ Project Structure

```
repo/
â”œâ”€â”€ manifests/
â”‚   â”œâ”€â”€ k8s-juiceds-pvc.yaml
â”‚   â”œâ”€â”€ ray-cluster.yaml
â”‚   â””â”€â”€ juiceds-runtimeclass.yaml
â”œâ”€â”€ ray/
â”‚   â”œâ”€â”€ train_gpt2_ray.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset-loader.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ upload-to-spaces.sh
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ğŸ› ï¸ Coming Soon

- Fine-tuning GPT models with Ray Train
- Integration with Hugging Face Transformers
- Tokenized dataset preprocessing